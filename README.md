# üè≠ InduSense - Big Data Lakehouse

## Mini-Projet: Simulation de flux de donn√©es de capteurs industriels

**Module:** Data Warehouse et Big Data Warehouse  
**Universit√©:** Sultan Moulay Slimane - ENSA Khouribga  
**Professeur:** M. Mostafa SAADI

---

## üìã Table des mati√®res

1. [Contexte du projet](#contexte-du-projet)
2. [Architecture](#architecture)
3. [Installation](#installation)
4. [Structure du projet](#structure-du-projet)
5. [Guide d'utilisation](#guide-dutilisation)
6. [D√©tail des composants](#d√©tail-des-composants)
7. [Analyses d√©cisionnelles](#analyses-d√©cisionnelles)
8. [Int√©gration Power BI](#int√©gration-power-bi)

---

## üéØ Contexte du projet

La soci√©t√© **InduSense** op√®re plusieurs sites industriels √©quip√©s de capteurs IoT qui collectent des mesures techniques (temp√©rature, pression, vibration). Ce projet simule cet environnement et construit un **Big Data Lakehouse** bas√© sur **Apache Spark** et **Delta Lake**.

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         ARCHITECTURE LAKEHOUSE                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  Capteur Temp    ‚îÇ     ‚îÇ  Capteur Vibr    ‚îÇ     ‚îÇ  Capteur Press   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  (Python)        ‚îÇ     ‚îÇ  (Python)        ‚îÇ     ‚îÇ  (Python)        ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ           ‚îÇ                        ‚îÇ                        ‚îÇ           ‚îÇ
‚îÇ           ‚ñº                        ‚ñº                        ‚ñº           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ                      DATA LAKE BRUT (Raw Zone)                      ‚îÇ‚îÇ
‚îÇ  ‚îÇ  /data_lake/raw/                                                    ‚îÇ‚îÇ
‚îÇ  ‚îÇ    ‚îú‚îÄ‚îÄ temperature/  (fichiers JSON)                                ‚îÇ‚îÇ
‚îÇ  ‚îÇ    ‚îú‚îÄ‚îÄ vibration/    (fichiers JSON)                                ‚îÇ‚îÇ
‚îÇ  ‚îÇ    ‚îî‚îÄ‚îÄ pressure/     (fichiers JSON)                                ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ                                    ‚îÇ                                     ‚îÇ
‚îÇ                                    ‚ñº                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ                    PIPELINE D'INT√âGRATION                           ‚îÇ‚îÇ
‚îÇ  ‚îÇ  (Apache Spark + Delta Lake)                                        ‚îÇ‚îÇ
‚îÇ  ‚îÇ                                                                      ‚îÇ‚îÇ
‚îÇ  ‚îÇ  1. Surveillance des r√©pertoires raw                                ‚îÇ‚îÇ
‚îÇ  ‚îÇ  2. Validation des donn√©es JSON                                     ‚îÇ‚îÇ
‚îÇ  ‚îÇ  3. Transformation et enrichissement                                ‚îÇ‚îÇ
‚îÇ  ‚îÇ  4. √âcriture en format Delta Lake                                   ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ                                    ‚îÇ                                     ‚îÇ
‚îÇ                                    ‚ñº                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ                      DATA LAKEHOUSE (Delta Lake)                    ‚îÇ‚îÇ
‚îÇ  ‚îÇ  /data_lake/warehouse/sensors/                                      ‚îÇ‚îÇ
‚îÇ  ‚îÇ                                                                      ‚îÇ‚îÇ
‚îÇ  ‚îÇ  Partitionnement: site / type / year / month / day                  ‚îÇ‚îÇ
‚îÇ  ‚îÇ  Format: Delta Lake (Parquet + Transaction Log)                     ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ                                    ‚îÇ                                     ‚îÇ
‚îÇ                                    ‚ñº                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ                    ANALYSES D√âCISIONNELLES                          ‚îÇ‚îÇ
‚îÇ  ‚îÇ  (Spark SQL)                                                        ‚îÇ‚îÇ
‚îÇ  ‚îÇ                                                                      ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Temp√©rature moyenne par site/machine                             ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Alertes critiques par type de capteur                            ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Top 5 variabilit√© vibration                                      ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ √âvolution horaire pression                                       ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ                                    ‚îÇ                                     ‚îÇ
‚îÇ                                    ‚ñº                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ                         REPORTING                                    ‚îÇ‚îÇ
‚îÇ  ‚îÇ  (Power BI / CSV Export)                                            ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ                                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üõ†Ô∏è Installation

### Pr√©requis

- Python 3.8+
- Java 8 ou 11 (pour Spark)
- Apache Spark 3.x
- Minimum 4 Go RAM

### Installation des d√©pendances

```bash
# Cloner ou t√©l√©charger le projet
cd indusense_lakehouse

# Cr√©er un environnement virtuel (recommand√©)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou: venv\Scripts\activate  # Windows

# Installer les d√©pendances
pip install -r requirements.txt
```

### Configuration Java (si n√©cessaire)

```bash
# V√©rifier la version de Java
java -version

# D√©finir JAVA_HOME si n√©cessaire
export JAVA_HOME=/path/to/java
```

---

## üìÅ Structure du projet

```
indusense_lakehouse/
‚îÇ
‚îú‚îÄ‚îÄ simulators/                    # Partie 1: Simulateurs de capteurs
‚îÇ   ‚îú‚îÄ‚îÄ temperature_sensor.py      # G√©n√®re des mesures de temp√©rature
‚îÇ   ‚îú‚îÄ‚îÄ vibration_sensor.py        # G√©n√®re des mesures de vibration
‚îÇ   ‚îú‚îÄ‚îÄ pressure_sensor.py         # G√©n√®re des mesures de pression
‚îÇ   ‚îî‚îÄ‚îÄ run_all_simulators.py      # Lance tous les simulateurs
‚îÇ
‚îú‚îÄ‚îÄ pipeline/                      # Partie 2: Pipeline d'int√©gration
‚îÇ   ‚îî‚îÄ‚îÄ lakehouse_pipeline.py      # Pipeline Spark + Delta Lake
‚îÇ
‚îú‚îÄ‚îÄ analysis/                      # Partie 3: Analyses Spark SQL
‚îÇ   ‚îî‚îÄ‚îÄ spark_analytics.py         # Requ√™tes analytiques
‚îÇ
‚îú‚îÄ‚îÄ data_lake/                     # Stockage des donn√©es
‚îÇ   ‚îú‚îÄ‚îÄ raw/                       # Zone brute (JSON)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ temperature/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vibration/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pressure/
‚îÇ   ‚îî‚îÄ‚îÄ warehouse/                 # Zone Lakehouse (Delta)
‚îÇ       ‚îî‚îÄ‚îÄ sensors/
‚îÇ
‚îú‚îÄ‚îÄ reports/                       # Rapports g√©n√©r√©s (CSV)
‚îú‚îÄ‚îÄ checkpoints/                   # Points de contr√¥le Spark
‚îú‚îÄ‚îÄ requirements.txt               # D√©pendances Python
‚îî‚îÄ‚îÄ README.md                      # Documentation
```

---

## üöÄ Guide d'utilisation

### √âtape 1: G√©n√©rer les donn√©es (Simulateurs)

```bash
cd simulators

# Lancer tous les simulateurs (1000 mesures par capteur)
python run_all_simulators.py

# Ou lancer un simulateur sp√©cifique
python temperature_sensor.py
python vibration_sensor.py
python pressure_sensor.py

# Avec un nombre personnalis√© de mesures
python run_all_simulators.py 5000
```

### √âtape 2: Ex√©cuter le Pipeline d'int√©gration

```bash
cd pipeline

# Mode batch (traite tous les fichiers existants)
python lakehouse_pipeline.py --mode batch

# Mode streaming (surveillance continue)
python lakehouse_pipeline.py --mode streaming

# Afficher les statistiques du warehouse
python lakehouse_pipeline.py --mode stats
```

### √âtape 3: Lancer les analyses

```bash
cd analysis

# Toutes les analyses
python spark_analytics.py --analyse all

# Analyses individuelles
python spark_analytics.py --analyse temperature
python spark_analytics.py --analyse alertes
python spark_analytics.py --analyse vibration
python spark_analytics.py --analyse pression

# Export pour Power BI
python spark_analytics.py --analyse export
```

---

## üì¶ D√©tail des composants

### 1. Simulateurs de capteurs

Chaque simulateur g√©n√®re des mesures au format JSON:

```json
{
  "sensor_id": "550e8400-e29b-41d4-a716-446655440000",
  "type": "temperature",
  "value": 45.67,
  "unit": "Celsius",
  "site": "Site_Paris",
  "machine": "Machine_A1",
  "timestamp": "2026-01-08T14:30:45.123456"
}
```

**Configuration des capteurs:**

| Capteur     | Unit√©   | Plage normale | Seuil critique |
|-------------|---------|---------------|----------------|
| Temp√©rature | Celsius | 20 - 80       | > 85           |
| Vibration   | mm/s    | 0.5 - 4.5     | > 7.0          |
| Pression    | bar     | 1.0 - 5.0     | > 6.0          |

### 2. Pipeline d'int√©gration

Le pipeline effectue les op√©rations suivantes:

1. **Surveillance**: D√©tecte les nouveaux fichiers JSON
2. **Validation**: V√©rifie la structure et la coh√©rence
3. **Transformation**:
   - Conversion des timestamps
   - Ajout des colonnes de partitionnement (year, month, day, hour)
   - Calcul des flags d'alerte
4. **Stockage**: √âcriture en format Delta Lake partitionn√©

**Partitionnement Delta Lake:**
```
/data_lake/warehouse/sensors/
  ‚îî‚îÄ‚îÄ site=Site_Paris/
      ‚îî‚îÄ‚îÄ type=temperature/
          ‚îî‚îÄ‚îÄ year=2026/
              ‚îî‚îÄ‚îÄ month=1/
                  ‚îî‚îÄ‚îÄ day=8/
                      ‚îî‚îÄ‚îÄ part-00000.parquet
```

### 3. Analyses Spark SQL

Quatre analyses principales sont impl√©ment√©es:

1. **Temp√©rature moyenne par site et machine**
2. **Alertes critiques par type de capteur**
3. **Top 5 variabilit√© de vibration**
4. **√âvolution horaire de la pression**

---

## üìä Analyses d√©cisionnelles

### Analyse 1: Temp√©rature moyenne

```sql
SELECT 
    site,
    machine,
    date_format(timestamp, 'yyyy-MM-dd') as date,
    ROUND(AVG(value), 2) as temperature_moyenne,
    COUNT(*) as nombre_mesures
FROM sensor_data
WHERE type = 'temperature'
GROUP BY site, machine, date_format(timestamp, 'yyyy-MM-dd')
ORDER BY site, machine, date
```

### Analyse 2: Alertes critiques

```sql
SELECT 
    type as type_capteur,
    COUNT(*) as total_mesures,
    SUM(CASE WHEN is_alert = true THEN 1 ELSE 0 END) as alertes_critiques,
    ROUND(SUM(CASE WHEN is_alert = true THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as pourcentage
FROM sensor_data
GROUP BY type
ORDER BY alertes_critiques DESC
```

### Analyse 3: Variabilit√© vibration

```sql
SELECT 
    machine,
    site,
    ROUND(STDDEV(value), 3) as ecart_type,
    ROUND(AVG(value), 3) as moyenne,
    COUNT(*) as mesures
FROM sensor_data
WHERE type = 'vibration'
GROUP BY machine, site
ORDER BY ecart_type DESC
LIMIT 5
```

### Analyse 4: √âvolution pression

```sql
SELECT 
    site,
    HOUR(timestamp) as heure,
    ROUND(AVG(value), 2) as pression_moyenne
FROM sensor_data
WHERE type = 'pressure'
GROUP BY site, HOUR(timestamp)
ORDER BY site, heure
```

---

## üìà Int√©gration Power BI

### Export des donn√©es

```bash
python analysis/spark_analytics.py --analyse export
```

G√©n√®re: `reports/powerbi_export.csv`

### Configuration Power BI

1. **Importer les donn√©es**: Fichier ‚Üí Obtenir des donn√©es ‚Üí CSV
2. **Cr√©er les visualisations**:
   - Graphique en courbes: √âvolution temporelle
   - Histogramme: Alertes par type
   - Carte: Distribution par site
   - Tableau: Top 5 machines

### Exemple de dashboard

| Visualisation | Type | Donn√©es |
|--------------|------|---------|
| Temp√©rature moyenne | Graphique en courbes | site, date, temp√©rature |
| Alertes critiques | Histogramme | type, count alertes |
| Top 5 vibration | Tableau | machine, √©cart-type |
| Pression horaire | Graphique en aires | heure, pression moyenne |

---

## üîß Troubleshooting

### Erreur: "Java not found"
```bash
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk
export PATH=$JAVA_HOME/bin:$PATH
```

### Erreur: "Delta Lake not found"
```bash
pip install delta-spark==3.0.0
```

### Probl√®me de m√©moire
```bash
export SPARK_DRIVER_MEMORY=4g
```

---

## üìù Auteur

Projet r√©alis√© dans le cadre du module **Data Warehouse et Big Data Warehouse**  
ENSA Khouribga - Fili√®re Informatique et Ing√©nierie de Donn√©es

---

## üìú Licence

Ce projet est √† but √©ducatif uniquement.
