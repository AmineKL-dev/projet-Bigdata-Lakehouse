#!/usr/bin/env python3
"""
Analytics Real-Time pour InduSense
Analyse continue des donn√©es du Data Lakehouse avec export pour Power BI
"""

import os
import logging
import time
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, avg, stddev, min as spark_min, max as spark_max, hour, sum as spark_sum
from delta import configure_spark_with_delta_pip

# ------------------------------------------------------------------
# LOGGING
# ------------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# ------------------------------------------------------------------
# PATHS
# ------------------------------------------------------------------
BASE_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
WAREHOUSE_PATH = os.path.join(BASE_PATH, "data_lake/warehouse/sensors")
REPORTS_PATH = os.path.join(BASE_PATH, "reports")
POWERBI_PATH = os.path.join(REPORTS_PATH, "powerbi_realtime")

os.makedirs(REPORTS_PATH, exist_ok=True)
os.makedirs(POWERBI_PATH, exist_ok=True)
os.makedirs("C:/stmp", exist_ok=True)

# ------------------------------------------------------------------
# ALERT THRESHOLDS
# ------------------------------------------------------------------
ALERT_THRESHOLDS = {
    "temperature": {"min": 0, "max": 85},
    "vibration": {"min": 0, "max": 7.0},
    "pressure": {"min": 0.5, "max": 6.0}
}


class RealtimeAnalytics:

    def __init__(self):
        logger.info("üöÄ Initialisation Spark Analytics Real-Time...")

        builder = (
            SparkSession.builder
            .appName("InduSense_Realtime_Analytics")
            .config("spark.driver.memory", "2g") \
            .config("spark.executor.memory", "2g") \
            .config("spark.sql.shuffle.partitions", "2") \
            .config("spark.ui.enabled", "false") \
            .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
            .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
            .config("spark.local.dir", "C:/stmp")
            .config("spark.driver.extraJavaOptions", "-Dlog4j2.formatMsgNoLookups=true")
            .config("spark.executor.extraJavaOptions", "-Dlog4j2.formatMsgNoLookups=true")
            .config("spark.ui.enabled", "false")
            .config("spark.driver.memory", "2g")
        )

        self.spark = configure_spark_with_delta_pip(builder).getOrCreate()
        self.spark.sparkContext.setLogLevel("ERROR")

        logger.info("‚úÖ Environnement pr√™t")

    # ------------------------------------------------------------------
    # CHARGEMENT DES DONN√âES (rafra√Æchi √† chaque appel)
    # ------------------------------------------------------------------
    def _load_data(self):
        """Charge les donn√©es depuis Delta Lake"""
        if not os.path.exists(WAREHOUSE_PATH):
            raise FileNotFoundError(f"‚ùå Warehouse introuvable: {WAREHOUSE_PATH}")

        self.df = self.spark.read.format("delta").load(WAREHOUSE_PATH)
        self.df.createOrReplaceTempView("sensor_data")
        
        return self.df.count()

    # ------------------------------------------------------------------
    # EXPORT CSV OPTIMIS√â (1 SEUL FICHIER)
    # ------------------------------------------------------------------
    def _export_single_csv(self, df, filename):
        """Exporte un DataFrame en UN SEUL fichier CSV"""
        temp_path = os.path.join(POWERBI_PATH, f"_{filename}_temp")
        final_path = os.path.join(POWERBI_PATH, f"{filename}.csv")
        
        # √âcrire dans un dossier temporaire
        (
            df.coalesce(1)
            .write
            .mode("overwrite")
            .option("header", "true")
            .csv(temp_path)
        )
        
        # Trouver le fichier CSV g√©n√©r√© (part-00000-*.csv)
        import glob
        csv_files = glob.glob(os.path.join(temp_path, "part-*.csv"))
        
        if csv_files:
            # D√©placer vers le fichier final
            import shutil
            shutil.move(csv_files[0], final_path)
            
            # Nettoyer le dossier temporaire
            shutil.rmtree(temp_path)
            
            logger.info(f"‚úÖ Export√©: {final_path}")
        else:
            logger.error(f"‚ùå Erreur export: aucun fichier CSV g√©n√©r√©")

    # ==================================================================
    # 1. DONN√âES BRUTES (pour visualisation Power BI)
    # ==================================================================
    def export_raw_data(self):
        """Exporte toutes les donn√©es brutes pour Power BI"""
        logger.info("üìä Export donn√©es brutes...")
        
        count = self._load_data()
        
        if count == 0:
            logger.warning("‚ö†Ô∏è  Aucune donn√©e √† exporter")
            return
        
        # Toutes les colonnes n√©cessaires
        df_export = self.spark.sql("""
            SELECT
                sensor_id,
                type,
                value,
                unit,
                site,
                machine,
                timestamp,
                year,
                month,
                day,
                hour,
                is_alert,
                ingestion_timestamp
            FROM sensor_data
            ORDER BY timestamp DESC
        """)
        
        self._export_single_csv(df_export, "raw_data")
        logger.info(f"   üìà {count} enregistrements export√©s")

    # ==================================================================
    # 2. STATISTIQUES PAR TYPE DE CAPTEUR
    # ==================================================================
    def export_stats_by_type(self):
        """Statistiques agr√©g√©es par type de capteur"""
        logger.info("üìä Export stats par type...")
        
        self._load_data()
        
        stats = self.spark.sql("""
            SELECT 
                type,
                COUNT(*) as total_mesures,
                ROUND(AVG(value), 2) as valeur_moyenne,
                ROUND(MIN(value), 2) as valeur_min,
                ROUND(MAX(value), 2) as valeur_max,
                ROUND(STDDEV(value), 2) as ecart_type,
                SUM(CASE WHEN is_alert THEN 1 ELSE 0 END) as nombre_alertes,
                MAX(timestamp) as derniere_mesure
            FROM sensor_data
            GROUP BY type
            ORDER BY type
        """)
        
        self._export_single_csv(stats, "stats_by_type")

    # ==================================================================
    # 3. STATISTIQUES PAR SITE
    # ==================================================================
    def export_stats_by_site(self):
        """Statistiques par site industriel"""
        logger.info("üìä Export stats par site...")
        
        self._load_data()
        
        stats = self.spark.sql("""
            SELECT 
                site,
                type,
                COUNT(*) as total_mesures,
                ROUND(AVG(value), 2) as valeur_moyenne,
                SUM(CASE WHEN is_alert THEN 1 ELSE 0 END) as alertes,
                MAX(timestamp) as derniere_mesure
            FROM sensor_data
            GROUP BY site, type
            ORDER BY site, type
        """)
        
        self._export_single_csv(stats, "stats_by_site")

    # ==================================================================
    # 4. STATISTIQUES PAR MACHINE
    # ==================================================================
    def export_stats_by_machine(self):
        """Statistiques par machine"""
        logger.info("üìä Export stats par machine...")
        
        self._load_data()
        
        stats = self.spark.sql("""
            SELECT 
                site,
                machine,
                type,
                COUNT(*) as total_mesures,
                ROUND(AVG(value), 2) as valeur_moyenne,
                SUM(CASE WHEN is_alert THEN 1 ELSE 0 END) as alertes,
                MAX(timestamp) as derniere_mesure
            FROM sensor_data
            GROUP BY site, machine, type
            ORDER BY site, machine, type
        """)
        
        self._export_single_csv(stats, "stats_by_machine")

    # ==================================================================
    # 5. √âVOLUTION HORAIRE
    # ==================================================================
    def export_hourly_trends(self):
        """√âvolution des mesures par heure"""
        logger.info("üìä Export tendances horaires...")
        
        self._load_data()
        
        trends = self.spark.sql("""
            SELECT 
                date_format(timestamp, 'yyyy-MM-dd') as date,
                HOUR(timestamp) as heure,
                type,
                COUNT(*) as nombre_mesures,
                ROUND(AVG(value), 2) as valeur_moyenne,
                SUM(CASE WHEN is_alert THEN 1 ELSE 0 END) as alertes
            FROM sensor_data
            GROUP BY date_format(timestamp, 'yyyy-MM-dd'), HOUR(timestamp), type
            ORDER BY date, heure, type
        """)
        
        self._export_single_csv(trends, "hourly_trends")

    # ==================================================================
    # 6. ALERTES CRITIQUES
    # ==================================================================
    def export_critical_alerts(self):
        """Export uniquement les alertes critiques"""
        logger.info("üìä Export alertes critiques...")
        
        self._load_data()
        
        alerts = self.spark.sql("""
            SELECT 
                sensor_id,
                type,
                value,
                unit,
                site,
                machine,
                timestamp,
                ingestion_timestamp
            FROM sensor_data
            WHERE is_alert = true
            ORDER BY timestamp DESC
        """)
        
        alert_count = alerts.count()
        
        if alert_count > 0:
            self._export_single_csv(alerts, "critical_alerts")
            logger.info(f"   üö® {alert_count} alertes export√©es")
        else:
            logger.info("   ‚úÖ Aucune alerte critique")

    # ==================================================================
    # 7. DASHBOARD SUMMARY (m√©triques cl√©s pour Power BI)
    # ==================================================================
    def export_dashboard_summary(self):
        """M√©triques cl√©s pour le dashboard Power BI"""
        logger.info("üìä Export r√©sum√© dashboard...")
        
        count = self._load_data()
        
        summary = self.spark.sql(f"""
            SELECT
                {count} as total_mesures,
                COUNT(DISTINCT site) as nombre_sites,
                COUNT(DISTINCT machine) as nombre_machines,
                SUM(CASE WHEN is_alert THEN 1 ELSE 0 END) as total_alertes,
                ROUND(SUM(CASE WHEN is_alert THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as taux_alerte_pct,
                MAX(timestamp) as derniere_mise_a_jour,
                NOW() as export_timestamp
            FROM sensor_data
        """)
        
        self._export_single_csv(summary, "dashboard_summary")

    # ==================================================================
    # EXPORT COMPLET (tous les fichiers)
    # ==================================================================
    def export_all(self):
        """Exporte tous les fichiers pour Power BI"""
        logger.info("=" * 60)
        logger.info("üìä EXPORT COMPLET POUR POWER BI")
        logger.info("=" * 60)
        
        start = time.time()
        
        try:
            self.export_raw_data()
            self.export_stats_by_type()
            self.export_stats_by_site()
            self.export_stats_by_machine()
            self.export_hourly_trends()
            self.export_critical_alerts()
            self.export_dashboard_summary()
            
            duration = time.time() - start
            
            logger.info("=" * 60)
            logger.info(f"‚úÖ Export termin√© en {duration:.2f}s")
            logger.info(f"üìÅ Fichiers disponibles dans: {POWERBI_PATH}")
            logger.info("=" * 60)
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'export: {e}")
            import traceback
            traceback.print_exc()

    # ==================================================================
    # MODE MONITORING CONSOLE (affichage temps r√©el)
    # ==================================================================
    def monitor_console(self, refresh_interval=10):
        """Affiche les statistiques en temps r√©el dans la console"""
        logger.info("=" * 60)
        logger.info("üìä MODE MONITORING CONSOLE")
        logger.info(f"‚è±Ô∏è  Rafra√Æchissement: {refresh_interval}s")
        logger.info("=" * 60)
        
        try:
            while True:
                count = self._load_data()
                
                if count == 0:
                    logger.warning("‚ö†Ô∏è  Aucune donn√©e disponible")
                    time.sleep(refresh_interval)
                    continue
                
                # Stats globales
                stats = self.spark.sql("""
                    SELECT 
                        type,
                        COUNT(*) as count,
                        ROUND(AVG(value), 2) as avg_value,
                        ROUND(MIN(value), 2) as min_value,
                        ROUND(MAX(value), 2) as max_value,
                        SUM(CASE WHEN is_alert THEN 1 ELSE 0 END) as alerts
                    FROM sensor_data
                    GROUP BY type
                    ORDER BY type
                """)
                
                # Derni√®res mesures
                recent = self.spark.sql("""
                    SELECT 
                        type, 
                        site, 
                        machine, 
                        ROUND(value, 2) as value,
                        CASE WHEN is_alert THEN 'üö®' ELSE '‚úÖ' END as status,
                        timestamp
                    FROM sensor_data
                    ORDER BY ingestion_timestamp DESC
                    LIMIT 10
                """)
                
                # Affichage
                os.system('cls' if os.name == 'nt' else 'clear')
                print("=" * 80)
                print(f"üìä MONITORING TEMPS R√âEL - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                print(f"üìà Total mesures: {count}")
                print("=" * 80)
                
                print("\nüìà Statistiques par type de capteur:")
                stats.show(truncate=False)
                
                print("\nüïê Derni√®res mesures:")
                recent.show(truncate=False)
                
                print(f"\n‚è±Ô∏è  Prochain rafra√Æchissement dans {refresh_interval}s... (Ctrl+C pour arr√™ter)")
                
                time.sleep(refresh_interval)
                
        except KeyboardInterrupt:
            logger.info("\nüõë Monitoring arr√™t√© par l'utilisateur")

    # ==================================================================
    # MODE AUTO-REFRESH (export automatique p√©riodique)
    # ==================================================================
    def auto_refresh_export(self, interval=30):
        """Export automatique p√©riodique pour Power BI"""
        logger.info("=" * 60)
        logger.info("üîÑ MODE AUTO-REFRESH POUR POWER BI")
        logger.info(f"‚è±Ô∏è  Intervalle: {interval}s")
        logger.info(f"üìÅ Destination: {POWERBI_PATH}")
        logger.info("=" * 60)
        
        iteration = 0
        
        try:
            while True:
                iteration += 1
                logger.info(f"\nüîÑ Export #{iteration} - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                
                self.export_all()
                
                logger.info(f"‚è±Ô∏è  Prochain export dans {interval}s... (Ctrl+C pour arr√™ter)\n")
                time.sleep(interval)
                
        except KeyboardInterrupt:
            logger.info("\nüõë Auto-refresh arr√™t√© par l'utilisateur")

    # ==================================================================
    def stop(self):
        """Arr√™te proprement la session Spark"""
        self.spark.stop()
        logger.info("üõë Spark arr√™t√© proprement")


# ==================================================================
# MAIN
# ==================================================================
def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="Analytics Real-Time pour InduSense - Export Power BI"
    )
    
    parser.add_argument(
        "--mode",
        choices=["export", "monitor", "auto-refresh"],
        default="export",
        help="Mode d'ex√©cution"
    )
    
    parser.add_argument(
        "--interval",
        type=int,
        default=30,
        help="Intervalle de rafra√Æchissement (secondes) pour auto-refresh"
    )
    
    parser.add_argument(
        "--monitor-interval",
        type=int,
        default=10,
        help="Intervalle d'affichage (secondes) pour le monitoring console"
    )

    args = parser.parse_args()
    analytics = RealtimeAnalytics()

    try:
        if args.mode == "export":
            # Export unique
            analytics.export_all()
            
        elif args.mode == "monitor":
            # Monitoring console en temps r√©el
            analytics.monitor_console(refresh_interval=args.monitor_interval)
            
        elif args.mode == "auto-refresh":
            # Export automatique p√©riodique pour Power BI
            analytics.auto_refresh_export(interval=args.interval)
            
    except Exception as e:
        logger.error(f"‚ùå Erreur: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        analytics.stop()


if __name__ == "__main__":
    main()
